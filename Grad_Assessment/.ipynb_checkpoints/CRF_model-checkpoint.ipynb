{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665e1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import math\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from scipy.linalg import expm\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdb328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF_model:\n",
    "    \n",
    "    def __init__(self, x_feats, y_feats, label_dict):\n",
    "        \n",
    "        #self.x: our np array of x features\n",
    "        #self.y: our np array of y features, which is simply the label IDs of a given sentence + BOS/EOS tokens\n",
    "        #self.ld: a dict containing labels as the keys and their respective IDs as the values\n",
    "        self.x = x_feats\n",
    "        self.y = y_feats\n",
    "        self.ld = label_dict\n",
    "    \n",
    "    def forward_vec():\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def backward_vec():\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #lamb is short for \"lambda\" (the parameter λ as described in CRFS: an introduction.) Lamb TBD using an optimizer \n",
    "    #function, such as Adam. Since most optimizers determine the minimum instead of maximum,\n",
    "    #simply optimize the negative likelihood instead of positive likelihood if using a minimum optimizer.\n",
    "    def log_likelihood(self, lamb):\n",
    "        \n",
    "        #label IDs for 'BOS' and 'EOS' tokens\n",
    "        BOS_ID = self.ld['BOS']\n",
    "        EOS_ID = self.ld['EOS']\n",
    "        \n",
    "        #computes Mi(y′, y|x) as described in section 6 of CRF introductions by Hannah. Used in log likelihood to \n",
    "        #compute Z(x)\n",
    "        #NOTE: can't use tf.math.reduce_logsumexp, since order is log -> exp -> sum instead of log -> sum -> exp\n",
    "        def compute_M(lamb_sum):\n",
    "            \n",
    "            return expm(lamb_sum)\n",
    "            \n",
    "        likelihood = 0\n",
    "        \n",
    "        #for each observation sequence and its respective label sequence within our x y datasets:\n",
    "        for x_feat, y_feat in zip(self.x, self.y):\n",
    "               \n",
    "            lamb_sum = 0\n",
    "            Z = None\n",
    "            \n",
    "            #for each word within our observation sequence:\n",
    "            for word in x_feat:\n",
    "\n",
    "                #lamb features: lambda (λ) * our X transition + state functions (Fj (y(k), x(k))) for a single sentence\n",
    "                #in our x features.\n",
    "                #lamb sum: the sum of lamb features over all of our x features. Used in computing Z\n",
    "\n",
    "                lamb_features = np.dot(word, lamb)\n",
    "                lamb_sum += lamb_features\n",
    "                if Z is None:\n",
    "                    Z = compute_M(lamb_features)\n",
    "                else:\n",
    "                    Z = np.matmul(Z, lamb_features)\n",
    "            try:    \n",
    "                likelihood += (lamb_sum - math.log(Z[BOS_ID] [EOS_ID]))\n",
    "            except ValueError:\n",
    "                print(\"ERROR: tried to log 0 when calculating Z\\(x\\)! Breaking function\")\n",
    "                break\n",
    "            \n",
    "        #Can be returned negative for optimizing purposes\n",
    "        return -likelihood\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        lamb_2 = np.random.randn(self.x[0].shape[3])\n",
    "        l = lambda lamb: self.log_likelihood(lamb)\n",
    "        #adadelta = tf.keras.optimizers.Adadelta(learning_rate = 0.001, rho = 0.95, epsilon = 1e-07, name = 'Adadelta')\n",
    "        \n",
    "        lamb_final = optimize.fmin_l_bfgs_b(l, lamb_2)\n",
    "        \n",
    "        #adadelta_run = adadelta.minimize(l, var_list = [lamb_final])\n",
    "        print(lamb_final)\n",
    "        \n",
    "        return lamb_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c1b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b928b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf3",
   "language": "python",
   "name": "tf3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
